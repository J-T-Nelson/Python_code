{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diff preprocessing of data for more understandable model evaluation \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "mmScaler = MinMaxScaler()\n",
    "X_3 = numeric.join(categorical)\n",
    "#print(X_3.info)\n",
    "XchrgVal = X_3['charges'].values\n",
    "print(XchrgVal, \"\\n\", type(XchrgVal), \"\\n\") # this is an ndArray\n",
    "\n",
    "Xcharges = X_3['charges']\n",
    "print(Xcharges , \"\\n\", type(Xcharges)) # this is a series\n",
    "\n",
    "\n",
    "Xtrmm, Xtsmm, ytrmm, ytsmm, = train_test_split(X_3.drop(columns='charges'), Xcharges, test_size=.2, random_state=1)\n",
    "print(Xtrmm.shape)\n",
    "print(Xtsmm.shape)\n",
    "\n",
    "Xtrmm = mmScaler.fit_transform(Xtrmm) # since we are scaling data before splitting we don't need to use the .fit_transform() then .transform() after.. we can just transform all the data.. \n",
    "# I wonder though, are there any benefits to splitting before you scale the data? or is it entirely optional? Just learned why they did what they did in the example \n",
    "Xtsmm = mmScaler.transform(Xtsmm)\n",
    "print(Xtrmm)\n",
    "print(Xtsmm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6fa2c8eb31e883fb5a22fbf67621de023b5dc228d46f0c7fcdad9efff2c48063"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
