{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 4 - Logistic Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#samples: 569, #features: 30\n",
      "[[1.799e+01 1.038e+01 1.228e+02 ... 2.654e-01 4.601e-01 1.189e-01]\n",
      " [2.057e+01 1.777e+01 1.329e+02 ... 1.860e-01 2.750e-01 8.902e-02]\n",
      " [1.969e+01 2.125e+01 1.300e+02 ... 2.430e-01 3.613e-01 8.758e-02]\n",
      " ...\n",
      " [1.660e+01 2.808e+01 1.083e+02 ... 1.418e-01 2.218e-01 7.820e-02]\n",
      " [2.060e+01 2.933e+01 1.401e+02 ... 2.650e-01 4.087e-01 1.240e-01]\n",
      " [7.760e+00 2.454e+01 4.792e+01 ... 0.000e+00 2.871e-01 7.039e-02]] \n",
      " <class 'numpy.ndarray'>\n",
      "\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 1 0 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 0 0 1 1 1 1 0 1 0 0\n",
      " 1 0 1 0 0 1 1 1 0 0 1 0 0 0 1 1 1 0 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1\n",
      " 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 0 0 1 0 1 0 0 1 0 0 1 1 0 1 1 0 1 1 1 1 0 1\n",
      " 1 1 1 1 1 1 1 1 0 1 1 1 1 0 0 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 1 1 0 0 0 1 0\n",
      " 1 0 1 1 1 0 1 1 0 0 1 0 0 0 0 1 0 0 0 1 0 1 0 1 1 0 1 0 0 0 0 1 1 0 0 1 1\n",
      " 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1 0 0 0 1 1\n",
      " 1 1 0 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0\n",
      " 0 1 0 0 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1\n",
      " 1 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1 0 1 1\n",
      " 0 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1\n",
      " 1 1 1 1 1 1 0 1 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0 1 1 0 1 0 1 0 0\n",
      " 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 0 0 0 0 0 0 1] \n",
      " <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from random import shuffle\n",
    "import random\n",
    "random.seed(123)\n",
    "\n",
    "X, y = datasets.load_breast_cancer(return_X_y=True)\n",
    "print(\"#samples: {}, #features: {}\".format(X.shape[0], X.shape[1]))\n",
    "\n",
    "\n",
    "print(X, '\\n', type(X))\n",
    "print('\\n',y, '\\n', type(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2179548934384 2179528304336\n",
      "False\n",
      "(569, 30)\n",
      "[[1.326e+03 8.474e-02]]\n",
      "[[1.228e+02 1.001e+03 1.184e-01]\n",
      " [1.329e+02 1.326e+03 8.474e-02]]\n"
     ]
    }
   ],
   "source": [
    "# practice slicing to remember: \n",
    "\n",
    "G = X.copy()\n",
    "print(id(G), id(X))\n",
    "print(id(G) == id(X)) # False \n",
    "\n",
    "print(G.shape)\n",
    "print(G[1:2, 3:5]) # should be seeing row 1-2 and col 3-5\n",
    "print(G[0:2, 2:5]) # forgot that python grabs the range specified -1 ... as the start is inclusive and the end is exclusive.. so its [2,5) .. werid but gotta remember.. otherwise the sytax is easy enough "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split and Normalize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_val: 500, test: 69\n",
      "<class 'numpy.ndarray'> (500, 30)\n"
     ]
    }
   ],
   "source": [
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.12, random_state=0)\n",
    "\n",
    "print(\"train_val: {}, test: {}\".format(X_train_val.shape[0], X_test.shape[0]))\n",
    "\n",
    "normalizer = StandardScaler()\n",
    "X_train_val = normalizer.fit_transform(X_train_val)\n",
    "X_test = normalizer.transform(X_test)\n",
    "\n",
    "print(type(X_train_val), X_train_val.shape[:]) # colon to get rows and cols. 0 for rows 1 for cols "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train log reg model and select hyper parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index_of_samples shape (500,)\n",
      "index of folds [[424 400 382 148  19 441 463 338 427 301  11 211  12 286 124 437 221 432\n",
      "  245 202 215 414  45 139 214 307  67 254 282 125  85  43 443  83 248 439\n",
      "  114 342 285 310 403 175 428 363 418 345 482 355 259 480 151 456 243 425\n",
      "  109 454 318 343 244  20 337 476 247  28 378  18 198 118  84 130 293 108\n",
      "  113  86 371 162 216 313 302 354  56 314  98 249 423 341 140  10 419 288\n",
      "  184 100 209 223 227 381 344  66  35 326]\n",
      " [411  96 389 450   8 466  73 468 191 388 491  95 421 104  57 217 264 387\n",
      "  212 165 379  91 188 396 335 292  89  61  16  88 496 194  93   3 452 290\n",
      "  250 412 474 393  17 306 449 295 361  44 116 279 347 119 230  72 445 204\n",
      "   13  49   5 246 272 263 377  29 495  94  23 166 150  87 121 224   6 172\n",
      "  232 107  54  42 304 384 241 176  74 117 126 174 208 275 484 179 461 284\n",
      "  251 228 218 258 333 105 145 252 328 143]\n",
      " [156  34 386  78 296 311   2 160 317 135 442 187 406  82 436 351 475 334\n",
      "  240 277 190 141 226 270 299 265 269 487 368 390 231 357 430 402 168 467\n",
      "  195 101 470 238 123 159  92  25 316  21 147  55 330 274 473 416 157 458\n",
      "  153  59 163 298 494 280 133 155 183 438 407  65 349 144 242  60 353 185\n",
      "  497 392 220 110  64 136 359  50 210 410 440 273 499 320 364  70 132 460\n",
      "  289  24 102 255 128   4 169  39 253 281]\n",
      " [391  48 429 398 448  53  62  37 236 321 129  30 112 260 127 152 492 404\n",
      "  213 167 315 287  31 325 177 380 106 122  79 276 271 137 486  26 233  69\n",
      "  324 323 331  40 493 401 278  38 234 180 182  41 142 373 120 164 189   7\n",
      "  170 455 483 329 385 138 267 201 261 350 154 346 472  52 103 235 283 149\n",
      "  336 413 358 134  51 131  58 193 383 352 332 171  76 348 462  36 257 305\n",
      "   81  27 372 262 479 239 178 498 366 339]\n",
      " [405 433 319  77  33  47 370 431 266 471   1 111 362  15 322 408 481 469\n",
      "  200 297 464 444  46 205  14 219  63 365 490 420 294  32 451 415  80 340\n",
      "  435 256 434 308 376 197 207 309 465  90 446  22 146   0 459 291 374 199\n",
      "  417 161 225 399 173 457 488  71 477 478 303  75 158 203 356 196 375 268\n",
      "  489  68 422 369 426 453 206 181 186 395 394 229 409 447 300 312 367 237\n",
      "    9 222 485 115  99 360 397  97 327 192]]\n",
      "index of folds shape (5, 100)\n",
      "reg_coeff: 10.0, acc: 0.966\n",
      "reg_coeff: 2.0, acc: 0.974\n",
      "reg_coeff: 1.0, acc: 0.972\n",
      "reg_coeff: 0.2, acc: 0.964\n",
      "reg_coeff: 0.1, acc: 0.964\n",
      "best_acc 0.9739999999999999 best_reg 0.5\n"
     ]
    }
   ],
   "source": [
    "# here we use 5-fold cross-validation\n",
    "folds = 5\n",
    "\n",
    "# get the number of samples in the training and validation set\n",
    "num_train_val = X_train_val.shape[0] \n",
    "\n",
    "# shuffle the index of samples in the train_val set\n",
    "index_of_samples = np.arange(num_train_val) \n",
    "shuffle(index_of_samples)\n",
    "\n",
    "print('index_of_samples shape', index_of_samples.shape)\n",
    "\n",
    "# split the index of the train_valid set into 5 folds\n",
    "index_of_folds = index_of_samples.reshape(folds, -1) # supplying '-1' infers the value of the second argument based on the length of the array and the remaining dimensions.\n",
    "print('index of folds', index_of_folds)\n",
    "print('index of folds shape', index_of_folds.shape)\n",
    "\n",
    "# index_of_foldsT = index_of_samples.reshape(folds, 100) # same output ... just not using inferred input\n",
    "# print('index of foldsT', index_of_foldsT)\n",
    "# print('index of folds shapeT', index_of_foldsT.shape)\n",
    "\n",
    "\n",
    "# potential hyperparameters. \n",
    "#These hyperparameters are just used for illustration. \n",
    "#You should try more hyperparameters to get a good model.\n",
    "#The hyperparameters must be nonnegative!\n",
    "regularization_coefficient = [0.1, 0.5, 1.0, 5.0, 10.0]\n",
    "\n",
    "best_acc = 0.0\n",
    "best_reg = 0.0\n",
    "\n",
    "for reg in regularization_coefficient:\n",
    "    \n",
    "    # 5-fold cross-validation\n",
    "    sum_acc = 0.0\n",
    "    for fold in range(folds):\n",
    "        \n",
    "        index_of_folds_temp = index_of_folds.copy()\n",
    "        \n",
    "        valid_index = index_of_folds_temp[fold,:].reshape(-1) #get the index of the validation set\n",
    "        train_index = np.delete(index_of_folds_temp, fold, 0).reshape(-1) #get the index of the training set\n",
    "     \n",
    "        # training set\n",
    "        X_train = X_train_val[train_index]\n",
    "        y_train = y_train_val[train_index]\n",
    "        \n",
    "        # validation set\n",
    "        X_valid = X_train_val[valid_index]\n",
    "        y_valid = y_train_val[valid_index]\n",
    "                \n",
    "        # build the model with different hyperparameters\n",
    "        clf = LogisticRegression(penalty='l2', C=reg, solver='lbfgs')\n",
    "        \n",
    "        #train the model with the training set\n",
    "        clf.fit(X_train, y_train)\n",
    "        \n",
    "        y_valid_pred = clf.predict(X_valid)\n",
    "        acc = accuracy_score(y_valid, y_valid_pred)\n",
    "        \n",
    "        sum_acc += acc\n",
    "    \n",
    "    cur_acc = sum_acc / folds\n",
    "    \n",
    "    print(\"reg_coeff: {}, acc: {:.3f}\".format(1.0/reg, cur_acc))\n",
    "    \n",
    "    # store the best hyperparameter\n",
    "    if cur_acc > best_acc:\n",
    "        best_acc = cur_acc\n",
    "        best_reg = reg\n",
    "        \n",
    "print('best_acc', best_acc, 'best_reg', best_reg) # this value is actually changing from one run to the next, suggesting more fine reg terms may be necessary to converge on one best value ... also suggests there is some inherent noise in the model which comes through in results.. it's not always producing the same accuracy measures and differs by as much as 3% I think "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the learned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 1.000, recall: 1.000, precision: 1.000, f1: 1.000,\n"
     ]
    }
   ],
   "source": [
    "# retrain the model\n",
    "clf = LogisticRegression(penalty='l2', C=best_reg, solver='lbfgs')\n",
    "clf.fit(X_train_val, y_train_val)\n",
    "\n",
    "# evaluate the model on the testing set\n",
    "y_test_pred = clf.predict(X_test)\n",
    "\n",
    "\n",
    "acc = accuracy_score(y_test, y_test_pred)\n",
    "f1 = f1_score(y_test, y_test_pred)\n",
    "recall = recall_score(y_test, y_test_pred)\n",
    "precision = precision_score(y_test, y_test_pred)\n",
    "\n",
    "\n",
    "print(\"accuracy: {:.3f}, recall: {:.3f}, precision: {:.3f}, f1: {:.3f},\".format(acc, recall, precision, f1)) # a perfect fucking model. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6fa2c8eb31e883fb5a22fbf67621de023b5dc228d46f0c7fcdad9efff2c48063"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
