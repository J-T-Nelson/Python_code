{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 9: Document Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\Programming\\Python_code\\PrinciplesOfDS_Course\\Labs\n"
     ]
    }
   ],
   "source": [
    "import os   \n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "\n",
    "data = pd.read_csv('./Data/BBC_News_Train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ArticleId                                               Text  Category\n",
      "0       1833  worldcom ex-boss launches defence lawyers defe...  business\n",
      "1        154  german business confidence slides german busin...  business\n",
      "2       1101  bbc poll indicates economic gloom citizens in ...  business\n",
      "3       1976  lifestyle  governs mobile choice  faster  bett...      tech\n",
      "4        917  enron bosses in $168m payout eighteen former e...  business\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1490 entries, 0 to 1489\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   ArticleId  1490 non-null   int64 \n",
      " 1   Text       1490 non-null   object\n",
      " 2   Category   1490 non-null   object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 35.0+ KB\n",
      "None\n",
      "(1490, 3)\n",
      "298.0\n"
     ]
    }
   ],
   "source": [
    "# Inspect data\n",
    "print(data.head())\n",
    "print(data.info())\n",
    "print(data.shape)\n",
    "\n",
    "print(1490*.2) # 298 A viable split value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1191, 3) (298, 3)\n"
     ]
    }
   ],
   "source": [
    "# split data\n",
    "data_train, data_test = data.iloc[:1191,], data.iloc[1192:,]\n",
    "print(data_train.shape, data_test.shape) # looks good "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Represent Docs with TF-IDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1191, 22453) (298, 22453)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#TF-IDF representation for each document\n",
    "vectorizer = TfidfVectorizer()\n",
    "data_train_vectors = vectorizer.fit_transform(data_train.Text)\n",
    "data_test_vectors = vectorizer.transform(data_test.Text) \n",
    "\n",
    "print(data_train_vectors.shape, data_test_vectors.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse._csr.csr_matrix'>\n",
      "22453\n",
      "['00' '000' '000bn' ... 'zuluaga' 'zurich' 'zvonareva']\n"
     ]
    }
   ],
   "source": [
    "# inspect contents of vectorized documents\n",
    "print(type(data_train_vectors))\n",
    "print(len(vectorizer.get_feature_names_out())) # 22,453\n",
    "print(vectorizer.get_feature_names_out()) \n",
    "# cols = terms, rows = docs \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN Document classification: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9101613867304245\n",
      "{'n_neighbors': 4}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "\n",
    "Xtr = data_train_vectors\n",
    "Ytr = data_train.Category\n",
    "\n",
    "Xte = data_test_vectors\n",
    "Yte = data_test.Category\n",
    "\n",
    "k_range = range(1, 5)\n",
    "param_grid = dict(n_neighbors=k_range)\n",
    "\n",
    "clf_knn =  KNeighborsClassifier(n_neighbors=1)\n",
    "\n",
    "grid = GridSearchCV(clf_knn, param_grid, cv=5, scoring='accuracy')\n",
    "grid.fit(Xtr, Ytr)\n",
    "\n",
    "print(grid.best_score_)\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "     business       0.94      0.85      0.89        68\n",
      "entertainment       0.93      0.89      0.91        64\n",
      "     politics       0.84      0.90      0.87        59\n",
      "        sport       0.94      0.98      0.96        60\n",
      "         tech       0.94      0.98      0.96        47\n",
      "\n",
      "     accuracy                           0.92       298\n",
      "    macro avg       0.92      0.92      0.92       298\n",
      " weighted avg       0.92      0.92      0.92       298\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "# running prediction and inspecting a classification report \n",
    "Yte_pred = grid.predict(Xte)\n",
    "print(classification_report(Yte, Yte_pred))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion of KNN: \n",
    "The data is fairly well balanced, with only techn being substantially lower in the number of samples available. The calssifier is pretty effective, which is in stark contrast to the KNN's results when used on the larger data set in the example provided to us for this lab. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Document Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 8}\n",
      "0.9731543624161074 0.9722015242878761 0.9731543624161074\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#=====training with cross validation======\n",
    "coeff = range(1, 10)\n",
    "param_grid = dict(C=coeff)\n",
    "\n",
    "clf_lr = LogisticRegression(penalty='l2')\n",
    "\n",
    "grid = GridSearchCV(clf_lr, param_grid, cv=5, scoring='accuracy')\n",
    "grid.fit(Xtr, Ytr)\n",
    "\n",
    "print(grid.best_params_)\n",
    "\n",
    "#=====testing======\n",
    "y_pred = grid.predict(Xte) # predicting with grid uses its best_estimator \n",
    "\n",
    "acc = accuracy_score(Yte, y_pred)\n",
    "macro_f1 = f1_score(Yte, y_pred, average='macro')\n",
    "micro_f1 = f1_score(Yte, y_pred, average='micro')\n",
    "\n",
    "print(acc, macro_f1, micro_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "     business       0.99      0.99      0.99        68\n",
      "entertainment       0.98      0.95      0.97        64\n",
      "     politics       0.96      0.93      0.95        59\n",
      "        sport       1.00      1.00      1.00        60\n",
      "         tech       0.92      1.00      0.96        47\n",
      "\n",
      "     accuracy                           0.97       298\n",
      "    macro avg       0.97      0.97      0.97       298\n",
      " weighted avg       0.97      0.97      0.97       298\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# classification report:\n",
    "print(classification_report(Yte, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kmeans Clustering for Document Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.cluster import normalized_mutual_info_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NMI for n_clusters = 4, init = k-means++, n_init = 20 ==> 0.46109288716601216\n",
      "NMI for n_clusters = 4, init = random, n_init = 20 ==> 0.5875981209449965\n",
      "NMI for n_clusters = 4, init = k-means++, n_init = 30 ==> 0.7302443838730827\n",
      "NMI for n_clusters = 4, init = random, n_init = 30 ==> 0.6756584824765176\n",
      "NMI for n_clusters = 4, init = k-means++, n_init = 50 ==> 0.6695005818206236\n",
      "NMI for n_clusters = 4, init = random, n_init = 50 ==> 0.61130035211045\n",
      "NMI for n_clusters = 4, init = k-means++, n_init = 100 ==> 0.558337446537098\n",
      "NMI for n_clusters = 4, init = random, n_init = 100 ==> 0.63475405486156\n",
      "NMI for n_clusters = 5, init = k-means++, n_init = 20 ==> 0.7411070400696318\n",
      "NMI for n_clusters = 5, init = random, n_init = 20 ==> 0.5583172344880103\n",
      "NMI for n_clusters = 5, init = k-means++, n_init = 30 ==> 0.5762626722615242\n",
      "NMI for n_clusters = 5, init = random, n_init = 30 ==> 0.6203761908596722\n",
      "NMI for n_clusters = 5, init = k-means++, n_init = 50 ==> 0.6664425704257937\n",
      "NMI for n_clusters = 5, init = random, n_init = 50 ==> 0.5972334331953142\n",
      "NMI for n_clusters = 5, init = k-means++, n_init = 100 ==> 0.6460116300040667\n",
      "NMI for n_clusters = 5, init = random, n_init = 100 ==> 0.8091369711645898\n",
      "NMI for n_clusters = 6, init = k-means++, n_init = 20 ==> 0.5495051591838367\n",
      "NMI for n_clusters = 6, init = random, n_init = 20 ==> 0.6300262640073588\n",
      "NMI for n_clusters = 6, init = k-means++, n_init = 30 ==> 0.5777924929698468\n",
      "NMI for n_clusters = 6, init = random, n_init = 30 ==> 0.6611664034835557\n",
      "NMI for n_clusters = 6, init = k-means++, n_init = 50 ==> 0.5801861783776473\n",
      "NMI for n_clusters = 6, init = random, n_init = 50 ==> 0.6793629801680944\n",
      "NMI for n_clusters = 6, init = k-means++, n_init = 100 ==> 0.607775193635488\n",
      "NMI for n_clusters = 6, init = random, n_init = 100 ==> 0.6212340186567838\n",
      "Best of each category:\n",
      "Best nmi 0.8091369711645898\n",
      "Best k cluster 5\n",
      "Best n_init 100\n",
      "Best init random\n"
     ]
    }
   ],
   "source": [
    "# Manual search for best hyperparameters using Kmeans: \n",
    "test_k = [4,5,6]\n",
    "test_n_init = [20, 30, 50, 100]\n",
    "test_init = ['k-means++', 'random']\n",
    "\n",
    "best_nmi = 0\n",
    "best_k = None\n",
    "best_n_init = None\n",
    "best_init = None\n",
    "\n",
    "for k in test_k:\n",
    "    for n in test_n_init:\n",
    "        for init in test_init:\n",
    "            temp_cluster = KMeans(n_clusters=k, init=init, n_init=n).fit(Xtr)\n",
    "            temp_nmi = normalized_mutual_info_score(temp_cluster.labels_, Ytr, average_method='arithmetic')\n",
    "            print(f\"NMI for n_clusters = {k}, init = {init}, n_init = {n} ==> {temp_nmi}\")\n",
    "            if(temp_nmi > best_nmi):\n",
    "                best_nmi = temp_nmi\n",
    "                best_k = k\n",
    "                best_n_init = n\n",
    "                best_init = init\n",
    "\n",
    "print(\"Best of each category:\\nBest nmi {}\\nBest k cluster {}\\nBest n_init {}\\nBest init {}\".format(best_nmi, best_k, best_n_init, best_init))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KMeans seems quite sensitive to the initialization state, I found an NMI of .82 on 5 clusters once with 50 = n_init, though its not consistantly able to produce that high result. So higher n_init == better outcome here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7742912350393736\n"
     ]
    }
   ],
   "source": [
    "# training KMeans after finding best params (higher n_init seems necessary to have higher probability of a good result):\n",
    "km_cluster = KMeans(n_clusters=5, init='k-means++', n_init=1000).fit(Xtr)\n",
    "km_nmi = normalized_mutual_info_score(km_cluster.labels_, Ytr, average_method='arithmetic')\n",
    "print(km_nmi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 1: ['the', 'and', 'to', 'in', 'film', 'of', 'best', 'for', 'on', 'was']\n",
      "Cluster 2: ['the', 'to', 'of', 'and', 'mr', 'he', 'in', 'labour', 'said', 'election']\n",
      "Cluster 3: ['the', 'to', 'of', 'and', 'in', 'that', 'is', 'it', 'are', 'mobile']\n",
      "Cluster 4: ['the', 'to', 'in', 'and', 'of', 'he', 'his', 'we', 'but', 'it']\n",
      "Cluster 5: ['the', 'to', 'of', 'in', 'and', 'said', 'its', 'that', 'it', 'for']\n"
     ]
    }
   ],
   "source": [
    "# Find top 10 representative words per cluster: \n",
    "centroids = km_cluster.cluster_centers_\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "for i in range(5):\n",
    "    indx = centroids[i].argsort()[-10:][::-1]\n",
    "    top_features = [feature_names[index] for index in indx]\n",
    "    print(f\"Cluster {i+1}: {top_features}\")\n",
    "    #print(f\"Values at indices: {centroids[i][indx]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
