{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "raw = pd.read_csv(\"D:\\\\Programming\\\\Python_code\\\\PrinciplesOfDS_Course\\\\Labs\\\\Data\\\\insurance.csv\")\n",
    "print(raw.info())\n",
    "print(raw.isna().sum()/raw.shape[0])\n",
    "\n",
    "#separate out categorical features from numeric\n",
    "categorical = raw.drop(columns=[\"age\", \"bmi\", \"charges\", \"children\"])\n",
    "print(categorical.info())\n",
    "\n",
    "numeric = raw.drop(columns=[\"sex\", \"smoker\", \"region\"])\n",
    "print(numeric.info())\n",
    "\n",
    "# checking object locations in memory:\n",
    "print(\"Displaying IDs: \\n\\n\\n\",id(raw), id(categorical), id(numeric))\n",
    "\n",
    "# anther way to check if two objects are the same in memory: \n",
    "print(\"\\n\",raw is categorical, raw is numeric, numeric is categorical, \"\\n\")\n",
    "\n",
    "# convert categorical features to numeric \n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "lenc = LabelEncoder()\n",
    "categorical = categorical.apply(lenc.fit_transform) # convert all categoricals to numeric using .apply()\n",
    "print(categorical.info())\n",
    "\n",
    "# Inspect unique values: \n",
    "#print(categorical.apply(unique)) # surprised I cannot use this syntax... I guess because the DF lacks the method.. maybe only series can have unique used on them? weird. \n",
    "\n",
    "print('Col 1:',categorical.iloc[:,0].unique(), 'Col 2:',categorical.iloc[:,1].unique(),'Col 3:',categorical.iloc[:,2].unique())\n",
    "#Col 1: [0 1] Col 2: [1 0] Col 3: [3 2 1 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recombining DFs, going to test the model on both non-normalized binary cols as well as normalized binary cols. \n",
    "\n",
    "#preProc = pd.concat([numeric,categorical], ignore_index=True, sort=False) # .append is deprecated.\n",
    "#print(preProc.info()) # important to note here, is whether or not any values would be shifted around, corrupting the data by mixing and matching values from different observations\n",
    "# so using concat actually added the new columns as new rows and didn't attmept to bind them together col by col.  \n",
    "\n",
    "preProc = numeric.join(categorical) # .join() works for just smashing two DFs together by index. \n",
    "print(\"Join version, uses indices as the key to join the two DFs from: \\n\", preProc.info())\n",
    "# not sure why, but the info() table prints before everything else.. which is honestly confusing. Will have to remember this abnormal behavior. \n",
    "\n",
    "x_mean_pd = preProc.mean() # I want to see what difference comes of using the DF method for mean vs a numpy method. \n",
    "x_std_pd = preProc.std()\n",
    "x_mean = np.mean(preProc, axis=0)\n",
    "x_std = np.std(preProc)\n",
    "\n",
    "print(\"\\nMethod differences. pandas mean output and type:\\n\", x_mean_pd, type(x_mean_pd), \"\\n\\nnumpy mean and output type:\\n\", x_mean, type(x_mean))\n",
    "# all the same. So it is safe to not use the numpy method I suppose which is nice as the syntax is cleaner for the pandas version\n",
    "\n",
    "print(\"\\nMethod differences. pandas std output and type:\\n\", x_std_pd, type(x_std_pd), \"\\n\\nnumpy std and output type:\\n\", x_std, type(x_std))\n",
    "# again no differences. So I think I will stick with the simpler syntax until I have reason to move on \n",
    "\n",
    "# Normalizing w/ Z-score standardization\n",
    "preProc = (preProc-x_mean)/x_std\n",
    "\n",
    "# creating non-normalized binary feature matrix\n",
    "preProcBinary = preProc.copy()\n",
    "preProcBinary['sex'] = categorical['sex']\n",
    "preProcBinary['smoker'] = categorical['smoker']\n",
    "\n",
    "print(preProc.head())\n",
    "print(preProcBinary.head())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Data into Training and Testing sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting cost, so dropping from feature matrix \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "feat = preProc.drop(columns=\"charges\")\n",
    "featBin = preProcBinary.drop(columns=\"charges\")\n",
    "\n",
    "charges = preProc['charges']\n",
    "chargesBin = preProcBinary['charges']\n",
    "\n",
    "# given my data is already standardized I don't think this normalization is necessary... \n",
    "# but I suppose I don't know how if effects things. Maybe can run code with and without this line to learn\n",
    "charges = charges / charges.max()\n",
    "chargesBin = chargesBin / chargesBin.max()\n",
    "\n",
    "Xtrain, Xtest, yTrain, yTest = train_test_split(feat, charges, test_size=0.2, random_state=1)\n",
    "XtrainBin, XtestBin, yTrainBin, yTestBin = train_test_split(featBin, chargesBin, test_size=0.2, random_state=1)\n",
    "\n",
    "print(Xtrain.shape, \"\\n\", Xtest.shape)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Linear Regression Model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "lr = LinearRegression()\n",
    "lrBin = LinearRegression()\n",
    "lr.fit(Xtrain, yTrain)\n",
    "lrBin.fit(XtrainBin, yTrainBin)\n",
    "\n",
    "print(\"Bias and Coefficient: \", str(lr.intercept_), str(lr.coef_)) \n",
    "print(\"Bias and Coefficient for Binary set: \", str(lrBin.intercept_), str(lrBin.coef_)) # bias is notably lower at -0.09411506075534107\n",
    "# coefficients are different on the binary values as well \n",
    "\n",
    "yTrainPred = lr.predict(Xtrain)\n",
    "yTrainPredBin = lrBin.predict(XtrainBin)\n",
    "\n",
    "mae = mean_absolute_error(yTrainPred, yTrain)\n",
    "maeBin = mean_absolute_error(yTrainPredBin, yTrainBin)\n",
    "mse = mean_squared_error(yTrainPred, yTrain)\n",
    "mseBin = mean_squared_error(yTrainPredBin, yTrainBin)\n",
    "rmse = np.sqrt(mse)\n",
    "rmseBin = np.sqrt(mseBin)\n",
    "\n",
    "print('prediction for training set:')\n",
    "print('MAE is: {}'.format(mae))\n",
    "print('MSE is: {}'.format(mse))\n",
    "print('RMSE is: {}'.format(rmse))\n",
    "\n",
    "print('prediction for Binary training set:')\n",
    "print('MAEBin is: {}'.format(maeBin))\n",
    "print('MSEBin is: {}'.format(mseBin))\n",
    "print('RMSEBin is: {}'.format(rmseBin))\n",
    "# predictions are totally equal despite different normalization here \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Lin Regression Model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "yTestPred = lr.predict(Xtest)\n",
    "yTestPredBin = lrBin.predict(XtestBin)\n",
    "\n",
    "mae = mean_absolute_error(yTestPred, yTest)\n",
    "maeBin = mean_absolute_error(yTestPredBin, yTestBin)\n",
    "mse = mean_squared_error(yTestPred, yTest)\n",
    "mseBin = mean_squared_error(yTestPredBin, yTestBin)\n",
    "rmse = np.sqrt(mse)\n",
    "rmseBin = np.sqrt(mseBin)\n",
    "\n",
    "print('prediction for training set:')\n",
    "print('MAE is: {}'.format(mae))\n",
    "print('MSE is: {}'.format(mse))\n",
    "print('RMSE is: {}'.format(rmse))\n",
    "\n",
    "print('prediction for Binary training set:')\n",
    "print('MAEBin is: {}'.format(maeBin))\n",
    "print('MSEBin is: {}'.format(mseBin))\n",
    "print('RMSEBin is: {}'.format(rmseBin))\n",
    "\n",
    "# We have 268 rows, so visualizing only a handful of results doesn't seem meaningful. \n",
    "labels = ['Patient_1','Patient_2','Patient_3','Patient_4','Patient_5','Patient_6','Patient_7','Patient_8','Patient_9','Patient_10']\n",
    "x = np.arange(len(labels))\n",
    "width = .25\n",
    "\n",
    "figure, axes = plt.subplots()\n",
    "bars1 = axes.bar(x-width/2, yTest[0:10], width, label='Ground Truth')\n",
    "bars2 = axes.bar(x+width/2, yTestPred[0:10], width, label='Prediciton')\n",
    "\n",
    "axes.set_ylabel('Charges')\n",
    "axes.set_xticks(x)\n",
    "axes.set_xticklabels(labels)\n",
    "axes.legend()\n",
    "\n",
    "plt.show()\n",
    "# my plot is looking pretty weird for a few reasons. Too many objects with too long of names for the labels to look good... \n",
    "# also the normalization that I did resulted in negative values which.. maybe is meaningful for the ability to have a good model, but visually/logically isn't helpful for understanding the outcomes. I don't really know what it means that some patients have charges which are negative "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making scatter plot for visualization: \n",
    "\n",
    "plt.scatter(yTest, yTestPred)\n",
    "plt.xlabel(\"Ground Truth Values\")\n",
    "plt.ylabel(\"Predicited Values\")\n",
    "plt.title(\"Lin Reg. Results\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "scatter = ax.scatter(yTest, yTestPred)\n",
    "ax.set_aspect('equal') # Chat GPT is lying or messed up... this isn't working for setting the scale of x and y equal \n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Min Max Scaled Data Work\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diff preprocessing of data for more understandable model evaluation \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "mmScaler = MinMaxScaler()\n",
    "X_3 = numeric.join(categorical)\n",
    "medicalFeatures = X_3.drop(columns='charges').values\n",
    "chrgVal = X_3['charges'].values\n",
    "\n",
    "\n",
    "Xtrmm, Xtsmm, ytrmm, ytsmm, = train_test_split(medicalFeatures, chrgVal, test_size=.2, random_state=1)\n",
    "print(Xtrmm.shape)\n",
    "print(Xtsmm.shape)\n",
    "\n",
    "Xtrmm = mmScaler.fit_transform(Xtrmm) # since we are scaling data before splitting we don't need to use the .fit_transform() then .transform() after.. we can just transform all the data.. \n",
    "# I wonder though, are there any benefits to splitting before you scale the data? or is it entirely optional? Just learned why they did what they did in the example \n",
    "Xtsmm = mmScaler.transform(Xtsmm) # looks like this transformation isn't quite working? \n",
    "print(Xtrmm)\n",
    "print(Xtsmm)\n",
    "\n",
    "print(\"\\n\\n\\n\\n\",ytrmm)\n",
    "print(ytsmm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing target variable for comparing results of non-normalized target vs normalized target building \n",
    "trgtScaler = MinMaxScaler()\n",
    "ytrmmScaled = trgtScaler.fit_transform(ytrmm.reshape(-1,1))\n",
    "ytsmmScaled = trgtScaler.transform(ytsmm.reshape(-1,1))\n",
    "\n",
    "#print(ytrmmScaled, ytsmmScaled) # values look good now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training new model on minmax Data\n",
    "lrmm = LinearRegression()\n",
    "lrmm.fit(Xtrmm, ytrmm)\n",
    "print(\"Bias:\", str(lrmm.intercept_))\n",
    "print(\"Weights/Coefficients:\", str(lrmm.coef_))\n",
    "\n",
    "yTrPred = lrmm.predict(Xtrmm)\n",
    "\n",
    "mae = mean_absolute_error(yTrPred,ytrmm)\n",
    "mse = mean_squared_error(yTrPred,ytrmm)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print('prediction for training set:')\n",
    "print('MAE is: {}'.format(mae))\n",
    "print('MSE is: {}'.format(mse))\n",
    "print('RMSE is: {}'.format(rmse))\n",
    "\n",
    "# all numbers are quite large right now, which confuses me, as the data going into the model should be pretty small overall. All minMax normalized. \n",
    "print(lrmm is lr, lrmm is lrBin, lrBin is lr) # none of my models are the same objects in memory... so that isn't why I am seeing weird numbers "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating MinMax Model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytsPred = lrmm.predict(Xtsmm)\n",
    "\n",
    "mae = mean_absolute_error(ytsPred,ytsmm)\n",
    "mse = mean_squared_error(ytsPred,ytsmm)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print('prediction for testing set:')\n",
    "print('MAE is: {}'.format(mae))\n",
    "print('MSE is: {}'.format(mse))\n",
    "print('RMSE is: {}'.format(rmse))\n",
    "\n",
    "# errors look quite similar... which is maybe good? Maybe its just outliers creating the big error.. but what is really strange to me is the fact that the numbers are so big.. the base dataset should be normalized and between 0 and 1 ... how do we produce such large errors? Maybe the linearRegression object is somehow de-normalizing my data? to give me numbers related to my original numbers? ... not sure, this mystery has got me confused but I will just move on for now. \n",
    "\n",
    "labels = ['Chrg_1','Chrg_2','Chrg_3','Chrg_4','Chrg_5','Chrg_6','Chrg_7','Chrg_8','Chrg_9','Chrg_10',]\n",
    "x = np.arange(len(labels))  # the label locations\n",
    "width = 0.45  # the width of the bars\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5)) # pylance AND general documentation for this don't have figsize as a parameter which can be adjusted.. wtf. How is one to learn about such features. \n",
    "rects1 = ax.bar(x - width/2, ytsmm[0:10], width, label='ground truth')\n",
    "rects2 = ax.bar(x + width/2, ytsPred[0:10], width, label='prediction')\n",
    "\n",
    "ax.set_ylabel('Charges')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.legend()\n",
    "\n",
    "plt.show() # bars look good, and the model is not horrible looking either"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making scatter plot to compare predicted values to ground truth values \n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "scatter = ax.scatter(ytsmm, ytsPred)\n",
    "ax.set_ylim(top=64000) # this worked for manually setting the limits on the y axis \n",
    "plt.show()\n",
    "\n",
    "# I would also like to see the R^2 value for this... obviously the model isn't so good. \n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "r2 = r2_score(ytsmm, ytsPred)\n",
    "print(\"R^2:\", r2) #R^2: 0.7623224022563388 ... honestly that isn't as bad as I expected. But what does it really say about the data? \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "regCoefficient = [.01, .05, .1, .25, .5, .75, 1, 2.5, 5, 7.5, 10] # 11 values to test\n",
    "\n",
    "# Standardized Variables: Xtrain, Xtest, yTrain, yTest\n",
    "# Standardized Variables untouched Binary values: XtrainBin, XtestBin, yTrainBin, yTestBin\n",
    "# MinMax variables with Normalized Target: Xtrmm, Xtsmm, ytrmmScaled, ytsmmScaled\n",
    "# MinMax variables non-Normalized Target: Xtrmm, Xtsmm, ytrmm, ytsmm\n",
    "\n",
    "#print(Xtrain, Xtest, yTrain, yTest)\n",
    "#print(XtrainBin, XtestBin, yTrainBin, yTestBin)\n",
    "#print(\"\\n\\n\\n\", Xtrmm, Xtsmm, ytrmm, ytsmm)\n",
    "#print(\"\\n\\n\\n\", Xtrmm, Xtsmm, ytrmmScaled, ytsmmScaled)\n",
    "\n",
    "# this loop gets the best reg value, then we use that reg value to make predictions on the testing data. \n",
    "best_r2 = 0\n",
    "best_tst_r2 = 0\n",
    "best_reg = 0 \n",
    "\n",
    "\n",
    "for reg in regCoefficient: \n",
    "    rr = Ridge(alpha=reg)\n",
    "    rr.fit(Xtrain, yTrain)\n",
    "\n",
    "    y_temp_pred = rr.predict(Xtrain)\n",
    "    y_tst_pred = rr.predict(Xtest)\n",
    "\n",
    "    # Bias = rr.intercept_\n",
    "    # weight = rr.coef_\n",
    "\n",
    "    mae = mean_absolute_error(y_temp_pred, yTrain)\n",
    "    mse = mean_squared_error(y_temp_pred, yTrain)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(yTrain, y_temp_pred)\n",
    "    #acc = accuracy_score(yTrain, y_temp_pred) # cannot get accuracy score for continuous values.. so we shall decide based upon the r2 score I suppose? \n",
    "\n",
    "    maeTst = mean_absolute_error(y_tst_pred, yTest)\n",
    "    mseTst = mean_squared_error(y_tst_pred, yTest)\n",
    "    rmseTst = np.sqrt(mseTst)\n",
    "    r2Tst = r2_score(yTest, y_tst_pred)\n",
    "\n",
    "    print('\\n\\nPrediction for training set: from reg:', reg, \"\\n\")\n",
    "    print('MAE is: {}, and for the test prediction its: {}'.format(mae, maeTst))\n",
    "    print('MSE is: {}, and for the test prediction its: {}'.format(mse, mseTst))\n",
    "    print('RMSE is: {}, and for the test prediction its: {}'.format(rmse, rmseTst))\n",
    "    print('r^2 is: {}, and for the test prediction its: {}'.format(r2, r2Tst))\n",
    "    #print('Accuracy Score is: {}'.format(acc))\n",
    "\n",
    "    if best_r2 < r2:\n",
    "        best_r2 = r2\n",
    "        best_tst_r2 = r2Tst\n",
    "        best_reg = reg\n",
    "\n",
    "print(\"\\n\\nBest r2, r2Tst and reg of all:\\n\", best_r2, best_tst_r2, best_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining function to improve testing efficiency: \n",
    "\n",
    "    # this function should take in preprocessed data and a parameter which determines which lin reg model to use\n",
    "    # it should return a named list (if that exists in python, maybe just a 1 x n data Frame?) which reports on the relevant values from each model, \n",
    "    # those ouputs can be funneled into a data Frame, which can than be hit with a graphing function to visualize all of the results. \n",
    "    # after results visulization the best model should be evident, and I can discuss it breifly for the lab. \n",
    "    # hypothetically I could recycle and build out these methods in the future in order to make testing of linear models more efficient for novel data sets. Would be fun to build this out across the semester. \n",
    "\n",
    "\n",
    "def linRegTester(X_train, X_test, y_train, y_test, model = \"lr\"):\n",
    "    pass\n",
    "\n",
    "    model_stats = dict()\n",
    "\n",
    "    if model == 'lr':\n",
    "        lr = LinearRegression()\n",
    "        lr.fit(X_train, y_train)\n",
    "\n",
    "        y_temp_pred = lr.predict(X_train)\n",
    "        y_tst_pred = lr.predict(X_test)\n",
    "\n",
    "        mae = mean_absolute_error(y_temp_pred, y_train)\n",
    "        mse = mean_squared_error(y_temp_pred, y_train)\n",
    "        rmse = np.sqrt(mse)\n",
    "        r2 = r2_score(y_train, y_temp_pred)\n",
    "\n",
    "        maeTst = mean_absolute_error(y_tst_pred, y_test)\n",
    "        mseTst = mean_squared_error(y_tst_pred, y_test)\n",
    "        rmseTst = np.sqrt(mseTst)\n",
    "        r2Tst = r2_score(y_test, y_tst_pred)\n",
    "\n",
    "        model_stats['mae'] = mae\n",
    "        model_stats['mse'] = mse\n",
    "        model_stats['rmse'] = rmse\n",
    "        model_stats['r2'] = r2\n",
    "        model_stats['maeTst'] = maeTst\n",
    "        model_stats['mseTst'] = mseTst\n",
    "        model_stats['rmseTst'] = rmseTst\n",
    "        model_stats['r2Tst'] = r2Tst\n",
    "        model_stats['best_r2'] = best_r2\n",
    "        model_stats['best_tst_r2'] = best_tst_r2\n",
    "        model_stats['best_reg'] = best_reg\n",
    "    \n",
    "\n",
    "    elif model == 'lasso':\n",
    "        regCoefficient = [.01, .05, .1, .25, .5, .75, 1, 2.5, 5, 7.5, 10]\n",
    "        best_r2 = 0\n",
    "        best_tst_r2 = 0\n",
    "        best_reg = 0\n",
    "        mae = 0.0\n",
    "        mse = 0.0\n",
    "        rmse = 0.0\n",
    "        maeTst = 0.0\n",
    "        mseTst = 0.0\n",
    "        rmseTst = 0.0\n",
    "\n",
    "        for reg in regCoefficient: \n",
    "            rr = Lasso(alpha=reg)\n",
    "            rr.fit(X_train, y_train)\n",
    "            \n",
    "            y_temp_pred = rr.predict(X_train)\n",
    "            y_tst_pred = rr.predict(X_test)\n",
    "            r2 = r2_score(y_train, y_temp_pred)\n",
    "            r2Tst = r2_score(y_test, y_tst_pred)\n",
    "\n",
    "            if best_r2 < r2:\n",
    "                best_r2 = r2\n",
    "                best_tst_r2 = r2Tst\n",
    "                best_reg = reg\n",
    "                mae = mean_absolute_error(y_temp_pred, y_train)\n",
    "                mse = mean_squared_error(y_temp_pred, y_train)\n",
    "                rmse = np.sqrt(mse)\n",
    "                maeTst = mean_absolute_error(y_tst_pred, y_test)\n",
    "                mseTst = mean_squared_error(y_tst_pred, y_test)\n",
    "                rmseTst = np.sqrt(mseTst)\n",
    "\n",
    "        model_stats['mae'] = mae\n",
    "        model_stats['mse'] = mse\n",
    "        model_stats['rmse'] = rmse\n",
    "        model_stats['r2'] = r2\n",
    "        model_stats['maeTst'] = maeTst\n",
    "        model_stats['mseTst'] = mseTst\n",
    "        model_stats['rmseTst'] = rmseTst\n",
    "        model_stats['r2Tst'] = r2Tst\n",
    "        model_stats['best_r2'] = None\n",
    "        model_stats['best_tst_r2'] = None\n",
    "        model_stats['best_reg'] = None\n",
    "\n",
    "\n",
    "    elif model == 'rr':\n",
    "        regCoefficient = [.01, .05, .1, .25, .5, .75, 1, 2.5, 5, 7.5, 10]\n",
    "        best_r2 = 0\n",
    "        best_tst_r2 = 0\n",
    "        best_reg = 0\n",
    "        mae = 0.0\n",
    "        mse = 0.0\n",
    "        rmse = 0.0\n",
    "        maeTst = 0.0\n",
    "        mseTst = 0.0\n",
    "        rmseTst = 0.0\n",
    "\n",
    "        for reg in regCoefficient: \n",
    "            rr = Ridge(alpha=reg)\n",
    "            rr.fit(X_train, y_train)\n",
    "\n",
    "            y_temp_pred = rr.predict(X_train)\n",
    "            y_tst_pred = rr.predict(X_test)\n",
    "            r2 = r2_score(y_train, y_temp_pred)\n",
    "            r2Tst = r2_score(y_test, y_tst_pred)\n",
    "\n",
    "            if best_r2 < r2:\n",
    "                best_r2 = r2\n",
    "                best_tst_r2 = r2Tst\n",
    "                best_reg = reg\n",
    "                mae = mean_absolute_error(y_temp_pred, y_train)\n",
    "                mse = mean_squared_error(y_temp_pred, y_train)\n",
    "                rmse = np.sqrt(mse)\n",
    "                maeTst = mean_absolute_error(y_tst_pred, y_test)\n",
    "                mseTst = mean_squared_error(y_tst_pred, y_test)\n",
    "                rmseTst = np.sqrt(mseTst)\n",
    "\n",
    "        model_stats['mae'] = mae\n",
    "        model_stats['mse'] = mse\n",
    "        model_stats['rmse'] = rmse\n",
    "        model_stats['r2'] = r2\n",
    "        model_stats['maeTst'] = maeTst\n",
    "        model_stats['mseTst'] = mseTst\n",
    "        model_stats['rmseTst'] = rmseTst\n",
    "        model_stats['r2Tst'] = r2Tst\n",
    "        model_stats['best_r2'] = None\n",
    "        model_stats['best_tst_r2'] = None\n",
    "        model_stats['best_reg'] = None\n",
    "\n",
    "\n",
    "    data_in = [X_train, X_test, y_train, y_test]\n",
    "    statsNdata = [data_in, model_stats]\n",
    "    return statsNdata\n",
    "\n",
    "#TODO: Test this function, then run it on the variations you want to run it on, after getting exported list of lists.. or w/e DS you store the returns in fokin graph that shit and then make up your report and submit it. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression Outcomes: \n",
    "\n",
    "on the training data AND the test data there is no significant difference for changeing the reg value at all with respect to the r2 produced. All r2's were practically identical at 74% . \n",
    "\n",
    "- Best r2 and reg of all:\n",
    " - 0.7475615631272967 0.01\n",
    "\n",
    "Similarly the MAE MSE and RMSE were all practically identical as well."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison Notes: \n",
    "\n",
    "I need to compare several different intersections of preprocessing techniques and models to draw insight from my work. Gonna list the intersections here: \n",
    "\n",
    "Preprocessing Techniques: \n",
    "1. Standardization, \n",
    "2. Standardization with Binary values untouched, \n",
    "3. MinMax normalization \n",
    "4. Min Max normalization with non-normalized target var. (ytrmm, ytsmm)\n",
    "\n",
    "Regression Models (techniques):\n",
    "1. Linear Regression\n",
    "2. Ridge Regression\n",
    "3. Lasso Regression \n",
    "\n",
    "3 x 4 = 12 => 12 different prediction sets to compare against one another. \n",
    "\n",
    "**IMPORTANTLY** We need to see how the model does on the training data versus the testing data. So we actually have 2x the reporting statistics to make up per intersection (I think)\n",
    "\n",
    "For 8 of the 12 we will be picking the best regularization coefficient and reporting it along side the prediction accuracy. (8 because there is no reg coefficient in the Linear Regression)\n",
    "\n",
    "Finally it would be nice to see all of these in a table and maybe put them into a simple bar graph which represents any key values which communicate about the efficacy of each model. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6fa2c8eb31e883fb5a22fbf67621de023b5dc228d46f0c7fcdad9efff2c48063"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
